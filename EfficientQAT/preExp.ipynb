{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/anaconda3/envs/quant/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.08715134859085083\n",
      "Epoch 2/10, Loss: 0.22687970101833344\n",
      "Epoch 3/10, Loss: 0.06562088429927826\n",
      "Epoch 4/10, Loss: 0.003095711348578334\n",
      "Epoch 5/10, Loss: 0.0029548597522079945\n",
      "Epoch 6/10, Loss: 0.018094034865498543\n",
      "Epoch 7/10, Loss: 0.01994013786315918\n",
      "Epoch 8/10, Loss: 0.001788316760212183\n",
      "Epoch 9/10, Loss: 0.008530102670192719\n",
      "Epoch 10/10, Loss: 0.07043803483247757\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the 3-layer MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.start_fc = nn.Linear(28 * 28, 256)\n",
    "        self.fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.lm_head = nn.Linear(256, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.relu(self.start_fc(x))\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.lm_head(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "def train(model, device, train_loader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLP().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, device, train_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/data/anaconda3/envs/quant/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.008553309366106987\n",
      "Epoch 2/10, Loss: 0.01526783686131239\n",
      "Epoch 3/10, Loss: 0.014119611121714115\n",
      "Epoch 4/10, Loss: 0.00949557963758707\n",
      "Epoch 5/10, Loss: 0.0013644912978634238\n",
      "Epoch 6/10, Loss: 0.0026759756729006767\n",
      "Epoch 7/10, Loss: 0.0006718770018778741\n",
      "Epoch 8/10, Loss: 0.000330804061377421\n",
      "Epoch 9/10, Loss: 7.87862609286094e-06\n",
      "Epoch 10/10, Loss: 4.950577931595035e-06\n",
      "Sample inp shape: torch.Size([5, 512])\n",
      "Sample tar shape: torch.Size([5, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# Define the 3-layer MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.start_fc = nn.Linear(28 * 28, 512)\n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, 512)\n",
    "        self.fc5 = nn.Linear(512, 512)\n",
    "        self.lm_head = nn.Linear(512, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.relu(self.start_fc(x))\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.relu(self.fc5(x))\n",
    "        x = self.lm_head(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Quantization function\n",
    "def quantize_tensor(x, num_bits=8):\n",
    "    qmin = 0.\n",
    "    qmax = 2. ** num_bits - 1.\n",
    "\n",
    "    min_val, max_val = x.min(), x.max()\n",
    "    scale = (max_val - min_val) / (qmax - qmin) if max_val != min_val else 1.0\n",
    "    zero_point = qmin - min_val / scale if scale != 0 else 0.\n",
    "\n",
    "    zero_point = zero_point.round().clamp(qmin, qmax)\n",
    "\n",
    "    q_x = (x / scale + zero_point).round().clamp(qmin, qmax)\n",
    "    # Dequantize\n",
    "    dq_x = (q_x - zero_point) * scale\n",
    "\n",
    "    return dq_x\n",
    "\n",
    "# Quantize specific layers in the model\n",
    "def quantize_model(model, num_bits=8):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if module.in_features == module.out_features:\n",
    "                # Quantize weights and biases\n",
    "                module.weight.data = quantize_tensor(module.weight.data, num_bits)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data = quantize_tensor(module.bias.data, num_bits)\n",
    "\n",
    "# Create a custom Dataset class\n",
    "class QuantizationDataset(data.Dataset):\n",
    "    def __init__(self, quant_model, raw_model, data_loader):\n",
    "        self.inp_list = []\n",
    "        self.tar_list = []\n",
    "\n",
    "        activation = {}\n",
    "\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                activation[name + '_input'] = input[0].detach()\n",
    "                activation[name + '_output'] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        # Register hooks for the layers to be quantized\n",
    "        raw_model.fc1.register_forward_hook(get_activation('raw_fc1'))\n",
    "        raw_model.fc2.register_forward_hook(get_activation('raw_fc2'))\n",
    "        raw_model.fc3.register_forward_hook(get_activation('raw_fc3'))\n",
    "        raw_model.fc4.register_forward_hook(get_activation('raw_fc4'))\n",
    "        raw_model.fc5.register_forward_hook(get_activation('raw_fc5'))\n",
    "        \n",
    "        quant_model.fc1.register_forward_hook(get_activation('quant_fc1'))\n",
    "        quant_model.fc2.register_forward_hook(get_activation('quant_fc2'))\n",
    "        quant_model.fc3.register_forward_hook(get_activation('quant_fc3'))\n",
    "        quant_model.fc4.register_forward_hook(get_activation('quant_fc4'))\n",
    "        quant_model.fc5.register_forward_hook(get_activation('quant_fc5'))\n",
    "\n",
    "        quant_model.eval()  # Set model to evaluation mode\n",
    "        raw_model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            for data, target in data_loader:\n",
    "                data = data.to(next(raw_model.parameters()).device)\n",
    "                output = raw_model(data)\n",
    "                _ = quant_model(data)\n",
    "                # Collect inputs and targets for each sample in the batch\n",
    "                batch_size = data.size(0)\n",
    "                for i in range(batch_size):\n",
    "                    inp_sample = []\n",
    "                    tar_sample = []\n",
    "                    for layer_name in ['fc1', 'fc2', 'fc3', 'fc4', 'fc5']:\n",
    "                        inp = activation[f'quant_{layer_name}_input'][i]\n",
    "                        quant_out = activation[f'quant_{layer_name}_output'][i]\n",
    "                        raw_out = activation[f'raw_{layer_name}_output'][i]\n",
    "                        inp_sample.append(inp.unsqueeze(0))\n",
    "                        tar_sample.append((raw_out - quant_out).unsqueeze(0))\n",
    "                    # Stack inputs and targets for the layers\n",
    "                    inp_sample = torch.cat(inp_sample, dim=0)\n",
    "                    tar_sample = torch.cat(tar_sample, dim=0)\n",
    "                    self.inp_list.append(inp_sample)\n",
    "                    self.tar_list.append(tar_sample)\n",
    "\n",
    "        # Stack all samples\n",
    "        self.inp = torch.stack(self.inp_list)\n",
    "        self.tar = torch.stack(self.tar_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.inp.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inp[idx], self.tar[idx]\n",
    "\n",
    "# Load MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset_raw = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader_raw = torch.utils.data.DataLoader(dataset=train_dataset_raw, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize model and device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLP().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Training loop\n",
    "def train(model, device, train_loader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "train(model, device, train_loader_raw, optimizer, criterion)\n",
    "raw_model = deepcopy(model)\n",
    "\n",
    "# Now quantize the model\n",
    "quantize_model(model, num_bits=8)\n",
    "quant_model = model\n",
    "\n",
    "# Create the quantization dataset\n",
    "quant_dataset = QuantizationDataset(quant_model,raw_model, train_loader_raw)\n",
    "quant_loader = torch.utils.data.DataLoader(dataset=quant_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "\n",
    "# You can now use quant_loader in your training loop or further processing\n",
    "# For demonstration, let's print the shapes of inp and tar for a sample\n",
    "sample_inp, sample_tar = quant_dataset[0]\n",
    "print(\"Sample inp shape:\", sample_inp.shape)  # Should be (3, 256)\n",
    "print(\"Sample tar shape:\", sample_tar.shape)  # Should be (3, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "Epoch 1 Train Loss: 6.5161741282145185\n",
      "Epoch 1 Validate Loss: 10.71863870493571\n",
      "Epoch 2 Train Loss: 4.642788636525472\n",
      "Epoch 2 Validate Loss: 7.987921098073324\n",
      "Epoch 3 Train Loss: 3.670166264216105\n",
      "Epoch 3 Validate Loss: 6.626560643513997\n",
      "Epoch 4 Train Loss: 3.1430958881378173\n",
      "Epoch 4 Validate Loss: 5.770802393595377\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model.mamba2 import Mamba2, Mamba2Config\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "print(len(quant_dataset))\n",
    "config = Mamba2Config(d_model=512, n_layers=1,d_head=4)\n",
    "model = Mamba2(config)\n",
    "model.to(device)\n",
    "train_set,validate_set = torch.utils.data.random_split(quant_dataset,[int(len(quant_dataset)*0.8),int(len(quant_dataset)*0.2)])\n",
    "train_loader = DataLoader(train_set,batch_size=32,shuffle=True)\n",
    "validate_loader = DataLoader(validate_set,batch_size=32,shuffle=True)\n",
    "learning_rate=1e-5\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                                                        optim,\n",
    "                                                        mode='min',\n",
    "                                                        factor=0.1, #factor by which the lr is multiplied\n",
    "                                                        patience=2,\n",
    "                                                    )\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for i,data in enumerate(train_loader):\n",
    "        inp,tar = data\n",
    "        inp = inp.to(device)\n",
    "        tar = tar.to(device)\n",
    "        optim.zero_grad()\n",
    "        output = model(inp)\n",
    "        loss = torch.norm(output - tar, p=2)\n",
    "        # loss = F.cross_entropy(output, tar)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_set)\n",
    "    print(f\"Epoch {epoch+1} Train Loss: {train_loss}\")\n",
    "    validate_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(validate_loader):\n",
    "            inp,tar = data\n",
    "            inp = inp.to(device)\n",
    "            tar = tar.to(device)\n",
    "            output = model(inp)\n",
    "            loss = torch.norm(output - tar, p=2)\n",
    "            # loss = F.cross_entropy(output, tar)\n",
    "            validate_loss += loss.item()\n",
    "    validate_loss /= len(validate_set)\n",
    "    scheduler.step(validate_loss)\n",
    "    print(f\"Epoch {epoch+1} Validate Loss: {validate_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0315,  0.0203,  0.0012,  ...,  0.0342, -0.0660,  0.0312],\n",
       "          [-0.0404, -0.0082,  0.0032,  ..., -0.0038,  0.0071, -0.0135],\n",
       "          [ 0.0554, -0.0059, -0.0388,  ...,  0.0272, -0.0205, -0.0064],\n",
       "          [-0.1671,  0.0236,  0.0662,  ...,  0.0980, -0.0178,  0.0068],\n",
       "          [-0.0216,  0.0113, -0.0448,  ..., -0.5089,  0.0084, -0.0216]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor([[-0.0101,  0.0003, -0.0046,  ...,  0.0043,  0.0126,  0.0145],\n",
       "         [ 0.0026, -0.0044,  0.0030,  ..., -0.0281, -0.0113, -0.0039],\n",
       "         [ 0.0003, -0.0164,  0.0151,  ...,  0.0216,  0.0093, -0.0016],\n",
       "         [-0.0074, -0.0056,  0.0211,  ...,  0.0101, -0.0070, -0.0012],\n",
       "         [-0.0271, -0.0073,  0.0089,  ...,  0.0050, -0.0241, -0.0065]],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp,tar=validate_set[0]\n",
    "inp.unsqueeze(0)\n",
    "out =model(inp.unsqueeze(0).to(device))\n",
    "out,tar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
