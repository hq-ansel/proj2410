# basic quantization settings
wbits: 2
group_size: 128
# training settings
cuda_ids: "0,1"
epochs: 1
batch_size: 1
quant_lr: 0.0001
weight_lr: 0.00001
off_load_to_disk: False
loss_func: MSE # "MSE", "FKLD" , "RKLD", "FKLD_RKLD" ,"MSE_FKLD", "MSE_RKLD", "MSE_FKLD_RKLD"
crossblock_window_size: 1
train_size: 4096


# save settings
log_loss: /home/ubuntu/data/exp/proj2410/logs/Llama2-7b-efficientqat-w2gs128-final_layer_denoise.csv
save_quant_dir: /home/ubuntu/data/exp/proj2410/quant_model/Llama2-7b/EfficientQAT/w2gs128-final_layer_denoise/
clamp_method: STE