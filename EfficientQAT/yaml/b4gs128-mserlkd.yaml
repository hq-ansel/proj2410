# basic quantization settings
wbits: 4
group_size: 128
# training settings
epochs: 5
batch_size: 8
off_load_to_disk: True
loss_func: MSE_RKLD # "MSE", "FKLD" , "RKLD", "FKLD_RKLD" ,"MSE_FKLD", "MSE_RKLD", "MSE_FKLD_RKLD"
crossblock_window_size: 1

# save settings
log_loss: /home/ubuntu/data/exp/proj2410/logs/Llama2-7b-efficientqatMSE_RKLD-w4gs128.csv
save_quant_dir: /home/ubuntu/data/exp/proj2410/quant_model/Llama2-7b/EfficientQATMSE_RKLD/w4gs128/
output_dir: /home/ubuntu/data/exp/proj2410/EfficientQAT/output/kld_exp/Llama-2-7b-w4g128