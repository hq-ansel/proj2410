# basic quantization settings
wbits: 2
group_size: 128
# training settings
cuda_ids: "1,3,4,5"
epochs: 5
rank: 4
world_size: 4
min_lr_factor: 20
net: Qwen2.5-0.5B
batch_size: 2
quant_lr: 0.0001
weight_lr: 0.0001
off_load_to_disk: False
loss_func: MSE # "MSE", "FKLD" , "RKLD", "FKLD_RKLD" ,"MSE_FKLD", "MSE_RKLD", "MSE_FKLD_RKLD"
crossblock_window_size: 1
train_size:  4096
slide_step: 1
clip_grad: 9999

quantizer_version: v3

model: /home/ubuntu/data/exp/proj2410/model/Qwen2.5-0.5B
# save settings
output_dir: /home/ubuntu/data/exp/proj2410/quant_model/Qwen-2.5-0.5B/EfficientQAT/w2gs128-v3/
log_loss: /home/ubuntu/data/exp/proj2410/quant_model/Qwen-2.5-0.5B/EfficientQAT/w2gs128-v3/loss.csv
save_quant_dir: /home/ubuntu/data/exp/proj2410/quant_model/Qwen-2.5-0.5B/EfficientQAT/w2gs128-v3/
clamp_method: STE

align_type: tail